import os
import sys
import torch
import torchvision
from torchvision.models import ResNet50_Weights
import torchvision.models as models
from torch.utils.data import DataLoader, random_split

# âœ… 1. è®¾ç½®è·¯å¾„
os.chdir(r"C:\\Users\\LBH\\Desktop\\PNIå®éªŒ Part II\\SYMH 1")
sys.path.append(os.getcwd())
print("ğŸ“‚ å½“å‰å·¥ä½œç›®å½•:", os.getcwd())

# âœ… 2. å¯¼å…¥ä½ è‡ªå·±çš„æ•°æ®åŠ è½½æ¨¡å—
from load_datasets2 import DatasetLoader

import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import roc_auc_score, roc_curve
from torch.optim.lr_scheduler import StepLR
import torch.nn.functional as F

# ====== è®­ç»ƒå‡½æ•° ======
def train(model, device, train_dataloader, optimizer, criterion, epoch, num_epochs):
    model.train()
    total_loss = 0
    total = 0
    correct = 0
    all_labels = []
    all_outputs = []
    for iter, (inputs, labels, filenames) in enumerate(train_dataloader):
        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = outputs > 0.5
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        all_labels.append(labels.detach().cpu().numpy())
        all_outputs.append(outputs.detach().cpu().numpy())

    train_accuracy = 100 * correct / total
    all_labels = np.concatenate(all_labels)
    all_outputs = np.concatenate(all_outputs)
    auc_score = roc_auc_score(all_labels, all_outputs)

    print(f"Epoch [{epoch}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Loss: {total_loss/len(train_dataloader):.4f}, AUC: {auc_score:.4f}")
    return train_accuracy, auc_score, all_labels, all_outputs


# ====== éªŒè¯å‡½æ•° ======
def test(model, device, test_dataloader, epoch, num_epochs):
    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_predictions = []
    all_filenames = []
    with torch.no_grad():
        for iter, (inputs, labels, filenames) in enumerate(test_dataloader):
            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)
            outputs = model(inputs)

            predicted = outputs > 0.5
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_labels.append(labels.cpu().numpy())
            all_predictions.append(outputs.cpu().numpy())
            all_filenames.extend(filenames)

    test_accuracy = correct / total * 100
    all_labels = np.concatenate(all_labels)
    all_predictions = np.concatenate(all_predictions)
    auc_score = roc_auc_score(all_labels, all_predictions)

    print(f"Epoch [{epoch}/{num_epochs}], Test Accuracy: {test_accuracy:.2f}%, AUC: {auc_score:.4f}")
    return test_accuracy, auc_score, all_labels, all_predictions, all_filenames


# ====== ä¸»ç¨‹åº ======
num_epochs = 200
lr = 1e-4
lambda_l2 = 1e-6
batch_size = 64
num_classes = 1

# âœ… è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
try:
    use_mps = torch.backends.mps.is_available()
except AttributeError:
    use_mps = False

if torch.cuda.is_available():
    device = "cuda"
elif use_mps:
    device = "mps"
else:
    device = "cpu"
print(f"âœ… Using device: {device}")

# âœ… åŠ è½½ CSV æ•°æ®é›†
csv_file = r"C:\\Users\\LBH\\Desktop\\PNIå®éªŒ Part II\\SYMH 1\\dataset.csv"
dataset = DatasetLoader(csv_file)

# âœ… å°†éªŒè¯é›†æ¯”ä¾‹æ”¹æˆ 40%
val_size = int(0.4 * len(dataset))
train_size = len(dataset) - val_size

# âœ… å›ºå®šéšæœºç§å­ç¡®ä¿åˆ’åˆ†ä¸€è‡´æ€§
torch.manual_seed(42)

TrainDataset, ValDataset = random_split(dataset, [train_size, val_size])

# âœ… ä¿å­˜åˆ’åˆ†ç´¢å¼•ï¼Œä¾›æ¨ç†è„šæœ¬å¤ç°ç›¸åŒè®­ç»ƒ/éªŒè¯é›†
split_dir = os.path.join(os.getcwd(), "checkpoint")
os.makedirs(split_dir, exist_ok=True)

np.save(os.path.join(split_dir, "train_indices.npy"), TrainDataset.indices)
np.save(os.path.join(split_dir, "val_indices.npy"), ValDataset.indices)
print(f"ğŸ’¾ å·²ä¿å­˜åˆ’åˆ†ç´¢å¼•æ–‡ä»¶ ({len(TrainDataset.indices)} train, {len(ValDataset.indices)} val)")

# âœ… æ„å»º DataLoader
TrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)
ValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)

# âœ… åˆå§‹åŒ– ResNet50 æ¨¡å‹
model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
in_features = model.fc.in_features
model.fc = nn.Sequential(
    nn.Linear(in_features, num_classes),
    nn.Sigmoid()
)
model.to(device)

# âœ… å®šä¹‰æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€LR è°ƒåº¦å™¨
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_l2)
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# âœ… æå‰åœæ­¢é…ç½®
early_stopping_rounds = 50
early_stopping_counter = 0
best_auc = 0.0

print(f"ğŸ“¦ Start training... total epochs = {num_epochs}")
for epoch in range(1, num_epochs + 1):
    train_accuracy, train_auc, train_labels, train_outputs = train(model, device, TrainDataLoader, optimizer, criterion, epoch, num_epochs)
    test_accuracy, auc_score, labels, predictions, filenames = test(model, device, ValDataLoader, epoch, num_epochs)
    scheduler.step()

    # âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹
    if auc_score > best_auc:
        best_auc = auc_score
        early_stopping_counter = 0
        best_model_path = os.path.join(os.getcwd(), "checkpoint", "best_model.pth")
        os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
        torch.save(model.state_dict(), best_model_path)
        print(f"ğŸ’¾ Saved new best model with AUC: {best_auc:.4f}")
    else:
        early_stopping_counter += 1

    if early_stopping_counter >= early_stopping_rounds:
        print(f"â¹ï¸ Early stopping at epoch {epoch}")
        break

print(f"ğŸ Training finished! Best AUC: {best_auc:.4f}")